services:
  # Development container for running tests and scripts
  dev:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: ser-dev
    volumes:
      # Mount source code for live editing
      - ./src:/app/src
      - ./apps:/app/apps
      - ./scripts:/app/scripts
      - ./tests:/app/tests
      - ./configs:/app/configs
      # Cache HuggingFace models to avoid re-downloading
      - huggingface-cache:/root/.cache/huggingface
      # Cache pip packages
      - pip-cache:/root/.cache/pip
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app:/app/src
    working_dir: /app
    stdin_open: true
    tty: true

  # FastAPI API (Phase 5 - production-ready REST API)
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: ser-api
    ports:
      - "8000:8000"
    volumes:
      # Cache HuggingFace models to avoid re-downloading
      - huggingface-cache:/root/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - APP_NAME=SER Service
      - LOG_LEVEL=INFO
      - MODEL_ID=baseline
      - DEVICE=cpu
      - MAX_DURATION_SEC=600
      - DEFAULT_WINDOW_SEC=2.0
      - DEFAULT_HOP_SEC=0.5
      - DEFAULT_PAD_MODE=zero
      - DEFAULT_SMOOTHING_METHOD=hysteresis
      - DEFAULT_HYSTERESIS_MIN_RUN=3
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')",
        ]
      interval: 10s
      timeout: 10s
      retries: 20
      start_period: 60s # Model loads faster when cached
    restart: unless-stopped

  # Legacy FastAPI Backend (moved to legacy/, kept for reference)
  # Uncomment if you need to run the legacy backend
  # backend:
  #   build:
  #     context: .
  #     dockerfile: legacy/backend/Dockerfile
  #   container_name: ser-backend
  #   ports:
  #     - "8001:8000"
  #   volumes:
  #     - huggingface-cache:/root/.cache/huggingface
  #   environment:
  #     - PYTHONUNBUFFERED=1
  #   restart: unless-stopped

  # Streamlit Frontend (with local inference capabilities)
  frontend:
    build:
      context: .
      dockerfile: apps/streamlit_app/Dockerfile
    container_name: ser-frontend
    ports:
      - "8501:8501"
    volumes:
      # Cache HuggingFace models to avoid re-downloading
      - huggingface-cache:/root/.cache/huggingface
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app:/app/src
    restart: unless-stopped

volumes:
  huggingface-cache:
    name: ser-huggingface-cache
  pip-cache:
    name: ser-pip-cache
